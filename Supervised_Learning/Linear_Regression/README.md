
# Simple Linear Regression: Computing Coefficients Using RSS

This guide explains how the coefficients `bâ‚€` (intercept) and `bâ‚` (slope) in simple linear regression are computed by minimizing the **Residual Sum of Squares (RSS)**.

---

## ğŸ“˜ Objective

In simple linear regression, we aim to model the relationship between a predictor variable `X` and a response variable `Y` using a straight line:

```
Å¶áµ¢ = bâ‚€ + bâ‚Xáµ¢
```

Where:
- `Å¶áµ¢`: Predicted value of `Y`
- `bâ‚€`: Intercept (value of `Y` when `X = 0`)
- `bâ‚`: Slope (change in `Y` per unit change in `X`)

---

## ğŸ¯ Minimizing the Residual Sum of Squares (RSS)

To find the best-fit line, we minimize the **Residual Sum of Squares**:

```
RSS = Î£(Yáµ¢ - Å¶áµ¢)Â² = Î£(Yáµ¢ - bâ‚€ - bâ‚Xáµ¢)Â²
```

RSS measures the total squared distance between the observed and predicted values.

---

## ğŸ§  Deriving the Coefficients

We minimize RSS by taking partial derivatives with respect to `bâ‚€` and `bâ‚`, and setting them to zero.

### ğŸ”¹ Step 1: Compute the Slope `bâ‚`

```
bâ‚ = Î£((Xáµ¢ - XÌ„)(Yáµ¢ - È²)) / Î£((Xáµ¢ - XÌ„)Â²)
```

This is the **slope** of the regression line. It reflects how much `Y` changes for a one-unit increase in `X`.

---

### ğŸ”¹ Step 2: Compute the Intercept `bâ‚€`

Once `bâ‚` is known, calculate the **intercept**:

```
bâ‚€ = È² - bâ‚XÌ„
```

This ensures the regression line passes through the point `(XÌ„, È²)`, the mean of the data.

---

## ğŸ” Summary of Final Formulas

- **Slope (bâ‚):**
```
bâ‚ = Î£((Xáµ¢ - XÌ„)(Yáµ¢ - È²)) / Î£((Xáµ¢ - XÌ„)Â²)
```

- **Intercept (bâ‚€):**
```
bâ‚€ = È² - bâ‚XÌ„
```

---

## ğŸ”¢ Example

Given:
- `X = [1, 2, 3]`
- `Y = [2, 4, 5]`

Steps:
1. Compute means: `XÌ„ = 2`, `È² â‰ˆ 3.67`
2. Calculate `bâ‚` using the slope formula.
3. Use `bâ‚€ = È² - bâ‚XÌ„` to get the intercept.

---

## âœ… Notes

- This approach assumes a linear relationship between `X` and `Y`.
- The goal is to minimize prediction error by finding the best-fit line using least squares.
- These formulas are the foundation for more advanced regression techniques.



## ğŸ“˜ Standard Errors in Simple Linear Regression

This document explains how to compute and interpret the **standard errors** of the regression coefficients in simple linear regression.

---

### ğŸ”¢ Equation

\[
\text{SE}(\hat{\beta}_0)^2 = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right), \quad
\text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
\]

- \(\hat{\beta}_0\): Estimated **intercept**
- \(\hat{\beta}_1\): Estimated **slope**
- \(\bar{x}\): Mean of the predictor variable \(x\)
- \(\sigma^2\): **Error variance**, estimated from residuals
- \(n\): Number of observations

---

### ğŸ“˜ What Do These Formulas Represent?

These equations give the **variances** of the least squares estimates of the slope and intercept. The **standard errors** (SE) are the square roots of these variances:

- \(\text{SE}(\hat{\beta}_0)\): measures uncertainty in the **intercept**
- \(\text{SE}(\hat{\beta}_1)\): measures uncertainty in the **slope**

---

### ğŸ§  Intuition Behind the Components

#### SE(\(\hat{\beta}_1\)) â€“ Slope

\[
\text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}
\]

- **Numerator**: \(\sigma^2\) = noise in the outcome variable \(y\)
- **Denominator**: variability in predictor \(x\)
- More spread in \(x\) â†’ more reliable slope estimate â†’ smaller SE

---

#### SE(\(\hat{\beta}_0\)) â€“ Intercept

\[
\text{SE}(\hat{\beta}_0)^2 = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2} \right)
\]

- **Includes two parts**:
  - \(1/n\): uncertainty from sample size
  - \(\bar{x}^2 / \sum (x_i - \bar{x})^2\): increases if \(\bar{x}\) is far from 0
- If \(\bar{x}\) is far from 0 or \(x\) is not spread out â†’ intercept becomes less reliable

---

### ğŸ“ˆ Interpretation Summary

| Term | Meaning | Grows When... |
|------|---------|----------------|
| \(\sigma^2\) | Variance of error terms | Data is noisy (high residuals) |
| \(\sum (x_i - \bar{x})^2\) | Spread of x-values | x-values are concentrated |
| \(\text{SE}(\hat{\beta}_1)\) | Slope uncertainty | x is not spread out or y is noisy |
| \(\text{SE}(\hat{\beta}_0)\) | Intercept uncertainty | \(\bar{x}\) is far from 0 or x is not spread |

---

### ğŸ§® Estimating \(\sigma^2\)

\[
\hat{\sigma}^2 = \frac{RSS}{n - 2}, \quad \text{where } RSS = \sum (y_i - \hat{y}_i)^2
\]

- RSS: Residual Sum of Squares (total squared error)
- \(n - 2\): Degrees of freedom (2 parameters: slope + intercept)

---

### âœ… TL;DR

- Standard errors help quantify how **reliable** your regression estimates are.
- More data, more spread in \(x\), and less noise in \(y\) â†’ smaller standard errors.
- These are essential for computing **confidence intervals** and **hypothesis tests** in regression.
